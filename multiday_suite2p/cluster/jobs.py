from multiday_suite2p.cluster.io  import test_extract_result_present
import uuid 
import re

from IPython import get_ipython
from pathlib import Path
from suite2p.io.server import ssh_connect


def extract_job(data_info, settings,data_path, force_recalc=False, bsubargs='', extract_job_path='~/.multiday-suite2p/extract_session_job.sh'):
    if (not test_extract_result_present(Path(data_info['data']['local_processed_root'])/data_info['data']['output_folder']/'sessions'/data_path)) | force_recalc:
        multiday_linux = (Path(data_info['data']['server_processed_root'])/data_info['data']['output_folder']).as_posix()
        data_folder = data_info['data']['server_processed_root']
        bin_folder = data_info['data']['server_bin_root']
        data_path = Path(data_path)
        server = settings['server']
        # load info.
        date = data_path.parts[-2]
        session = data_path.parts[-1]
        # cluster parameters.
        job_id = f'{date}-{session}-{uuid.uuid4().hex[:3].upper()}'
        n_cores = server['n_cores']
        # connect to ssh
        ssh = ssh_connect(server['host'], server['username'], server['password'],verbose=False)
        run_command =f'bsub -n {n_cores} -J {job_id} {bsubargs} -o logs/extract-{job_id}.txt "{extract_job_path}'
        # arguments.
        run_command+= f' \'{multiday_linux}\''
        run_command+= f' \'{data_folder}\''
        run_command+= f' \'{bin_folder}\''
        run_command+= f' \'{data_path.as_posix()}\''
        # log location
        run_command+= f' > logs/log-extract-{job_id}.txt"'
        stdin, stdout, stderr = ssh.exec_command(run_command)
        # find job id.
        stdout = stdout.read().decode('utf-8')
        stdout = re.search('Job <(\d*)>',stdout)
        if stdout:
            job_id = int(stdout.group(1))
            print(f'{data_path} - job: {job_id}')
            return {'data_path':data_path,'job_id':job_id}
        else:
            raise NameError("Could not find job id (was job submit succesfull?)")
    else:
        print(f'{data_path} - Already present')
    return

def check_job_status(job,data_info, settings):
    """Checks on status of job generated by send_session_job.

    Args:
        job (dictionary): Must contain:
                        'job_id': Job id on cluster.
                        'data_path': data_path of job (animal/date/session_id)
        server (dictionary): Necessary to connect to cluster.
                        'host'
                        'username'
                        'password'
    """
    server = settings['server']
    ssh = ssh_connect(server['host'], server['username'], server['password'],verbose=False)
    # check job status
    stdin, stdout, stderr = ssh.exec_command(f'bjobs -l { job["job_id"] }')
    status = re.search("Status <.*>",stdout.read().decode('utf-8')).group()
    # check combined folder 
    result_folder = Path(data_info['data']['local_processed_root'])/data_info['data']['output_folder']/'sessions'/job["data_path"]
    if test_extract_result_present(result_folder):
        result_folder = 'present'
    else:
        result_folder = 'absent'
    print(f'{job["data_path"]}: {status}, trace files: {result_folder}')

def extract_job_slurm(data_info, settings, data_path, force_recalc=False, sbatchargs='', extract_job_path='~/.multiday-suite2p/extract_session_job.sh'):
    if (not test_extract_result_present(Path(data_info['data']['local_processed_root'])/data_info['data']['output_folder']/'sessions'/data_path)) | force_recalc:
        multiday_linux = (Path(data_info['data']['server_processed_root'])/data_info['data']['output_folder']).as_posix()
        data_folder = data_info['data']['server_processed_root']
        bin_folder = data_info['data']['server_bin_root']
        data_path = Path(data_path)
        server = settings['server']
        # load info.
        date = data_path.parts[-2]
        session = data_path.parts[-1]
        # cluster parameters.
        job_id = f'{date}-{session}-{uuid.uuid4().hex[:3].upper()}'
        n_cores = server['n_cores']
        # connect to ssh
        ssh = ssh_connect(server['host'], server['username'], server['password'], verbose=True)
        run_command = f'sbatch -n {n_cores} -J "{job_id}" {sbatchargs} -o "logs/extract-{job_id}.txt" --wrap="{extract_job_path}'
        # arguments.
        run_command += f" '{multiday_linux}'"
        run_command += f" '{data_folder}'"
        run_command += f" '{bin_folder}'"
        run_command += f" '{data_path.as_posix()}'"
        # log location
        run_command += f' > logs/log-extract-{job_id}.txt"'
        stdin, stdout, stderr = ssh.exec_command(run_command)
        # find job id.
        stdout = stdout.read().decode('utf-8')
        stdout = re.search('Submitted batch job (\d*)', stdout)
        if stdout:
            job_id = int(stdout.group(1))
            print(f'{data_path} - job: {job_id}')
            return {'data_path': data_path, 'job_id': job_id}
        else:
            raise NameError("Could not find job id (was job submit successful?)")
    else:
        print(f'{data_path} - Already present')
    return

def check_job_status_slurm(job, data_info, settings):
    """Checks on status of job generated by send_session_job.

    Args:
        job (dictionary): Must contain:
                        'job_id': Job id on cluster.
                        'data_path': data_path of job (animal/date/session_id)
        server (dictionary): Necessary to connect to cluster.
                        'host'
                        'username'
                        'password'
    """
    server = settings['server']
    ssh = ssh_connect(server['host'], server['username'], server['password'], verbose=False)
    # check job status
    stdin, stdout, stderr = ssh.exec_command(f'scontrol show job {job["job_id"]}')
    status_output = stdout.read().decode('utf-8')
    status_match = re.search(r"JobState=(\w+)", status_output)
    status = f"Status <{status_match.group(1)}>" if status_match else "Status <Unknown>"
    # check combined folder 
    result_folder = Path(data_info['data']['local_processed_root'])/data_info['data']['output_folder']/'sessions'/job["data_path"]
    if test_extract_result_present(result_folder):
        result_folder = 'present'
    else:
        result_folder = 'absent'
    print(f'{job["data_path"]}: {status}, trace files: {result_folder}')


def extract_job_ipython(data_info, settings, data_path, force_recalc=False, sbatchargs='', extract_job_path='~/.multiday-suite2p/extract_session_job.sh'):
    # Check if previous extraction result exists or force recalculation is requested
    job_result = Path(data_info['data']['local_processed_root']) / data_info['data']['output_folder'] / 'sessions' / data_path
    if (not test_extract_result_present(job_result)) or force_recalc:
        # Build necessary paths and variables from data_info and settings
        multiday_linux = (Path(data_info['data']['server_processed_root']) / data_info['data']['output_folder']).as_posix()
        data_folder = data_info['data']['server_processed_root']
        bin_folder = data_info['data']['server_bin_root']
        data_path = Path(data_path)
        date = data_path.parts[-2]
        session = data_path.parts[-1]
        job_id = f'{date}-{session}-{uuid.uuid4().hex[:3].upper()}'
        n_cores = settings['server']['n_cores']

        # Construct the command line (note that the trailing " has been removed)
        run_command = (
            f'sbatch -n {n_cores} -J "{job_id}" {sbatchargs} -o logs/extract-{job_id}.txt '
            f'--wrap="source {extract_job_path}'
            f" '{multiday_linux}'"
            f" '{data_folder}'"
            f" '{bin_folder}'"
            f" '{data_path.as_posix()}'"
            f' > logs/log-extract-{job_id}.txt"'
        )
        print("Running command:", run_command)

        # Execute the command using the IPython system call; the output is captured as a list of strings
        output = get_ipython().getoutput(run_command)
        output_str = "\n".join(output)
        match = re.search(r"Submitted batch job (\d+)", output_str)
        if match:
            submitted_job_id = int(match.group(1))
            print(f'{data_path} - job: {submitted_job_id}')
            return {'data_path': str(data_path), 'job_id': submitted_job_id}
        else:
            raise NameError("Could not find job id (was job submit successful?)")
    else:
        print(f'{data_path} - Already present')
    return

def check_job_status_ipython(job, data_info):
    """
    Checks on the status of a job generated by extract_job.
    It runs the scontrol command locally using the IPython system call.
    """
    command = f'scontrol show job {job["job_id"]}'
    print("Checking job status with:", command)
    output = get_ipython().getoutput(command)
    output_str = "\n".join(output)
    status_match = re.search(r"JobState=(\w+)", output_str)
    status = f"Status <{status_match.group(1)}>" if status_match else "Status <Unknown>"

    result_folder = Path(data_info['data']['local_processed_root']) / data_info['data']['output_folder'] / 'sessions' / job["data_path"]
    file_status = 'present' if test_extract_result_present(result_folder) else 'absent'
    print(f'{job["data_path"]}: {status}, trace files: {file_status}')
